## Try to Ensure Explainability

By their nature, deep neural networks produce outcomes that are very difficult to interpret. While it can be easy to see how accurate the models are, it is rarely possible to understand why they make the decisions that they do. For the most part this has simply been accepted as a downside worth tolerating, but there is a growing push for explainable AI with lots of new research in this area. Wherever possible, a team should try to ensure as much explainability as possible, making it easier to see bias within the model. For example, there have been reports of robot judges in the US being inherently racist due to the data they have been trained on, and being more likely to hand down harsh sentences to African Americans and also label them as more likely to re-offend[1]. This is one example where explainability would be important. If it's clear that these decisions are a result of biased training data, it's much easier to stop this unfair process from happening. 

## Always Document Biases and Shortcomings

There will be times when bias is just a part of the process and is not avoidable. In these cases, a team should document them and perhaps explain why they are there and how they are being accounted for. It doesn't mean that the project is particularly flawed or unethical, it's just the nature of certain data sources. The problem arises if there is an attempt made to hide the biases of shortcomings and just pretend they don't exist.

## Evaluate whether the Model can be Used for Harm

As Deep Learning becomes more and more advanced and the potential impact that the trained models can have on society as a whole grows larger and larger, it is important to truly evaluate whether or not these models should be made publically available. In June 2020, The team at OpenAI created a language model that can create human-like text (GPT-3). They decided, however, that acccess would be tightly controlled and the general public would not be given unrestricted access to it. They figured that the potential harm, with fake news generationg etc. was simply to great. Users need to join a waitlist[2] and describe, in detail, what their use case will be. This is a good example of a team realising the potential harm their model could cause and strongly considering the ethics behind making it public. 

#### References

1 - [Rise of the racist robots â€“ how AI is learning all our worst impulses](https://www.theguardian.com/inequality/2017/aug/08/rise-of-the-racist-robots-how-ai-is-learning-all-our-worst-impulses)

2 - [OpenAI GPT-3 Waitlist](https://share.hsforms.com/1Lfc7WtPLRk2ppXhPjcYY-A4sk30)
